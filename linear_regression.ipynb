{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "training_epoch = 10000\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train graph input\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Variables\n",
    "\n",
    "W = tf.Variable(np.random.randn(), dtype = tf.float32)\n",
    "b = tf.Variable(np.random.randn(), dtype = tf.float32)\n",
    "\n",
    "# initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "# linear model\n",
    "\n",
    "linear_model = W*X + b\n",
    "error = tf.reduce_sum(tf.square(Y - linear_model))/(2*n_samples)\n",
    "\n",
    "# build an optimizer\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# minimize entry\n",
    "gradient = optimizer.minimize(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost after :  0 th iteration 128.059\n",
      "Training cost after :  50 th iteration inf\n",
      "Training cost after :  100 th iteration nan\n",
      "Training cost after :  150 th iteration nan\n",
      "Training cost after :  200 th iteration nan\n",
      "Training cost after :  250 th iteration nan\n",
      "Training cost after :  300 th iteration nan\n",
      "Training cost after :  350 th iteration nan\n",
      "Training cost after :  400 th iteration nan\n",
      "Training cost after :  450 th iteration nan\n",
      "Training cost after :  500 th iteration nan\n",
      "Training cost after :  550 th iteration nan\n",
      "Training cost after :  600 th iteration nan\n",
      "Training cost after :  650 th iteration nan\n",
      "Training cost after :  700 th iteration nan\n",
      "Training cost after :  750 th iteration nan\n",
      "Training cost after :  800 th iteration nan\n",
      "Training cost after :  850 th iteration nan\n",
      "Training cost after :  900 th iteration nan\n",
      "Training cost after :  950 th iteration nan\n",
      "Training cost after :  1000 th iteration nan\n",
      "Training cost after :  1050 th iteration nan\n",
      "Training cost after :  1100 th iteration nan\n",
      "Training cost after :  1150 th iteration nan\n",
      "Training cost after :  1200 th iteration nan\n",
      "Training cost after :  1250 th iteration nan\n",
      "Training cost after :  1300 th iteration nan\n",
      "Training cost after :  1350 th iteration nan\n",
      "Training cost after :  1400 th iteration nan\n",
      "Training cost after :  1450 th iteration nan\n",
      "Training cost after :  1500 th iteration nan\n",
      "Training cost after :  1550 th iteration nan\n",
      "Training cost after :  1600 th iteration nan\n",
      "Training cost after :  1650 th iteration nan\n",
      "Training cost after :  1700 th iteration nan\n",
      "Training cost after :  1750 th iteration nan\n",
      "Training cost after :  1800 th iteration nan\n",
      "Training cost after :  1850 th iteration nan\n",
      "Training cost after :  1900 th iteration nan\n",
      "Training cost after :  1950 th iteration nan\n",
      "Training cost after :  2000 th iteration nan\n",
      "Training cost after :  2050 th iteration nan\n",
      "Training cost after :  2100 th iteration nan\n",
      "Training cost after :  2150 th iteration nan\n",
      "Training cost after :  2200 th iteration nan\n",
      "Training cost after :  2250 th iteration nan\n",
      "Training cost after :  2300 th iteration nan\n",
      "Training cost after :  2350 th iteration nan\n",
      "Training cost after :  2400 th iteration nan\n",
      "Training cost after :  2450 th iteration nan\n",
      "Training cost after :  2500 th iteration nan\n",
      "Training cost after :  2550 th iteration nan\n",
      "Training cost after :  2600 th iteration nan\n",
      "Training cost after :  2650 th iteration nan\n",
      "Training cost after :  2700 th iteration nan\n",
      "Training cost after :  2750 th iteration nan\n",
      "Training cost after :  2800 th iteration nan\n",
      "Training cost after :  2850 th iteration nan\n",
      "Training cost after :  2900 th iteration nan\n",
      "Training cost after :  2950 th iteration nan\n",
      "Training cost after :  3000 th iteration nan\n",
      "Training cost after :  3050 th iteration nan\n",
      "Training cost after :  3100 th iteration nan\n",
      "Training cost after :  3150 th iteration nan\n",
      "Training cost after :  3200 th iteration nan\n",
      "Training cost after :  3250 th iteration nan\n",
      "Training cost after :  3300 th iteration nan\n",
      "Training cost after :  3350 th iteration nan\n",
      "Training cost after :  3400 th iteration nan\n",
      "Training cost after :  3450 th iteration nan\n",
      "Training cost after :  3500 th iteration nan\n",
      "Training cost after :  3550 th iteration nan\n",
      "Training cost after :  3600 th iteration nan\n",
      "Training cost after :  3650 th iteration nan\n",
      "Training cost after :  3700 th iteration nan\n",
      "Training cost after :  3750 th iteration nan\n",
      "Training cost after :  3800 th iteration nan\n",
      "Training cost after :  3850 th iteration nan\n",
      "Training cost after :  3900 th iteration nan\n",
      "Training cost after :  3950 th iteration nan\n",
      "Training cost after :  4000 th iteration nan\n",
      "Training cost after :  4050 th iteration nan\n",
      "Training cost after :  4100 th iteration nan\n",
      "Training cost after :  4150 th iteration nan\n",
      "Training cost after :  4200 th iteration nan\n",
      "Training cost after :  4250 th iteration nan\n",
      "Training cost after :  4300 th iteration nan\n",
      "Training cost after :  4350 th iteration nan\n",
      "Training cost after :  4400 th iteration nan\n",
      "Training cost after :  4450 th iteration nan\n",
      "Training cost after :  4500 th iteration nan\n",
      "Training cost after :  4550 th iteration nan\n",
      "Training cost after :  4600 th iteration nan\n",
      "Training cost after :  4650 th iteration nan\n",
      "Training cost after :  4700 th iteration nan\n",
      "Training cost after :  4750 th iteration nan\n",
      "Training cost after :  4800 th iteration nan\n",
      "Training cost after :  4850 th iteration nan\n",
      "Training cost after :  4900 th iteration nan\n",
      "Training cost after :  4950 th iteration nan\n",
      "Training cost after :  5000 th iteration nan\n",
      "Training cost after :  5050 th iteration nan\n",
      "Training cost after :  5100 th iteration nan\n",
      "Training cost after :  5150 th iteration nan\n",
      "Training cost after :  5200 th iteration nan\n",
      "Training cost after :  5250 th iteration nan\n",
      "Training cost after :  5300 th iteration nan\n",
      "Training cost after :  5350 th iteration nan\n",
      "Training cost after :  5400 th iteration nan\n",
      "Training cost after :  5450 th iteration nan\n",
      "Training cost after :  5500 th iteration nan\n",
      "Training cost after :  5550 th iteration nan\n",
      "Training cost after :  5600 th iteration nan\n",
      "Training cost after :  5650 th iteration nan\n",
      "Training cost after :  5700 th iteration nan\n",
      "Training cost after :  5750 th iteration nan\n",
      "Training cost after :  5800 th iteration nan\n",
      "Training cost after :  5850 th iteration nan\n",
      "Training cost after :  5900 th iteration nan\n",
      "Training cost after :  5950 th iteration nan\n",
      "Training cost after :  6000 th iteration nan\n",
      "Training cost after :  6050 th iteration nan\n",
      "Training cost after :  6100 th iteration nan\n",
      "Training cost after :  6150 th iteration nan\n",
      "Training cost after :  6200 th iteration nan\n",
      "Training cost after :  6250 th iteration nan\n",
      "Training cost after :  6300 th iteration nan\n",
      "Training cost after :  6350 th iteration nan\n",
      "Training cost after :  6400 th iteration nan\n",
      "Training cost after :  6450 th iteration nan\n",
      "Training cost after :  6500 th iteration nan\n",
      "Training cost after :  6550 th iteration nan\n",
      "Training cost after :  6600 th iteration nan\n",
      "Training cost after :  6650 th iteration nan\n",
      "Training cost after :  6700 th iteration nan\n",
      "Training cost after :  6750 th iteration nan\n",
      "Training cost after :  6800 th iteration nan\n",
      "Training cost after :  6850 th iteration nan\n",
      "Training cost after :  6900 th iteration nan\n",
      "Training cost after :  6950 th iteration nan\n",
      "Training cost after :  7000 th iteration nan\n",
      "Training cost after :  7050 th iteration nan\n",
      "Training cost after :  7100 th iteration nan\n",
      "Training cost after :  7150 th iteration nan\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(training_epoch):\n",
    "        sess.run(gradient, {X: train_X, Y: train_Y})\n",
    "        if i%display_step == 0:\n",
    "            print (\"Training cost after : \", i  ,'th iteration', sess.run(error, feed_dict = {X: train_X, Y: train_Y}))\n",
    "        \n",
    "    print (sess.run([W,b]))\n",
    "    print (\"Training cost : \", sess.run(error, feed_dict = {X: train_X, Y: train_Y}))\n",
    "    plt.plot(train_X, train_Y, 'ro')\n",
    "    plt.plot(train_X, sess.run(W)*train_X + sess.run(b), color = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.GradientDescentOptimizer.minimize?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
